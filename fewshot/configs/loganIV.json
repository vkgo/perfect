{
    "task": "rte",
    "max_seq_length": 256,
    "per_device_train_batch_size": 1,
    "per_device_eval_batch_size": 1,
    "K": 16,
    "data_dir": "datasets_processed",
    "output_dir": "outputs",
    "model_name_or_path": "roberta-large",
    "do_train": true,
    "do_eval": true,
    "do_predict": true,
    "max_steps": 6000, 
    "eval_steps": 100, 
    "save_steps": 100,
    "data_seed": 100, 
    "seed": 1, 
    "load_best_model_at_end": true,
    "metric_for_best_model": "average",
    "greater_is_better": true,
    "evaluation_strategy": "steps",
    "save_strategy":"steps",
    "save_total_limit": 1,
    "overwrite_output_dir": true,
    "overwrite_cache": true,
    "gradient_accumulation_steps": 32,
    "vectorize_pet": true,
    "with_augmentation": true,
    "learning_rate": 1e-4,
    "no_pattern": true,
    "tune_biases": true,
    "tune_layernorms": true,
    "add_layer_norm_after_adapter": false,
    "add_layer_norm_before_adapter": false,
    "add_adapter_after_attention": false, 
    "add_adapter_after_feedforward": true,
    "mask_position": "1"
}
